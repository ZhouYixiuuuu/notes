# 基础知识补充

## 什么是LSM-Tree？

下图展示了LevelDB中基于LSM-Tree的存储结构。

![image-20241024133419607](https://raw.githubusercontent.com/ZhouYixiuuuu/picture/master/imgs/202410241334864.png)

LevelDB的存储包括**内存数据库（MemTable)和磁盘数据库(SSTable)两部分**。

* 新增、修改、删除数据时首先会生成一条日志，追加在磁盘WAL文件（Write Ahead LOG）中，用于系统崩溃时进行数据还原

* 随后将操作（增删改）与数据信息编码成一条记录插入内存中的MemTable中，并对插入数据进行排序，MemTable是一个跳表结构（支持多步查询的有序链表），其插入时间复杂度为O(log n)

* 当MemTabel容量达到阈值时转化为Imutable MemTable，并创建一个新的MemTable用于后续数据写入

* 后台线程将Imumutable MemTable中的数据Flush到磁盘文件中SSTable文件（这个过程叫做Minor Compact)

  **MemTable中数据是有序的因此SSTable文件中数据也具有有序性**

* **LevelDB中数据可能会存储于MemTable、Immutable MemTable、SSTabel（Level 0~Level n）中，并且由于是追加写的操作，同一个Key可能同时存储在上述多个结构中**，只是这些Key对应的Value以及操作（增删改）不同。

读取：

* 从新文件往旧文件查询：先查内存数据Memtable和Immutable MemTable，若没查到，继续查找SSTable文件。
* 内存中MemTable和Imutable MemTable是跳表结构，查找时间复杂度为O(log n)，而**磁盘中第0层的SSTable文件之间可能存在数据重叠的情况，因此在数据查询时需要遍历Level 0的所有SSTable文件**
* 为了避免0层文件越来越庞大冗余影响数据读取效率以及磁盘空间利用率，当0层文件容量或数量达到一定阈值后，将通过**多路归并**的方式将第0层的文件合并为若干个**没有重叠**且有序的1层文件。
* 单个文件内部是有序的，可以采用二分查找的方式提升文件查询效率。

### 读/写/空间放大

* 读放大：每次读请求带来的读盘次数
* 写放大：

### compaction 策略

* size-tiered compaction：每层允许的SST文件最大数量都有个**相同的阈值**

  ![image-20241024145053958](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241024145053958.png)

  缺点：单个SST的大小有可能会很大，较高的层级出现数百GB甚至TB级别的SST文件都是常见的。
  

### 什么是跳表？

无锁跳表？

### LevelDB源码阅读

#### 写操作

1. 多个并发write合并成一个batch写入log和memtable的机制是怎么样的？

   *第一部分*

   ```c++
   // 创建写任务
     Writer w(&mutex_);
     w.batch = updates;
     w.sync = options.sync;
     w.done = false;
   ```

   *第二部分*

   ```c++
   // 获取mutex_互斥锁
   MutexLock l(&mutex_);
   ```

   *第三部分*

   ```c++
   writers_.push_back(&w);
   while (!w.done && &w != writers_.front()) {
       w.cv.Wait();
   }
   if (w.done) {
       return w.status;
   }
   ```

   *第四部分*（写前检查+获取操作序列号+buildbatchgroup）

   ```c++
   // 下面所有操作都是队长才能干的了
     // 写前检查
     Status status = MakeRoomForWrite(updates == nullptr);
     // 获取操作序列号
     uint64_t last_sequence = versions_->LastSequence();
     Writer* last_writer = &w;
   
     if (status.ok() && updates != nullptr) {  // nullptr batch is for compactions
       // 将几个写任务合在一起，由队长完成
       WriteBatch* write_batch = BuildBatchGroup(&last_writer);
       WriteBatchInternal::SetSequence(write_batch, last_sequence + 1);
       last_sequence += WriteBatchInternal::Count(write_batch);
   ```

   *第五部分*

   ```c++
   mutex_.Unlock();
   status = log_->AddRecord(WriteBatchInternal::Contents(write_batch));
   bool sync_error = false;
   if (status.ok() && options.sync) {
       status = logfile_->Sync();
       if (!status.ok()) {
           sync_error = true;
       }
   }
   if (status.ok()) {
       status = WriteBatchInternal::InsertInto(write_batch, mem_);
   }
   ```
   *第六部分*

   ```c++
   mutex_.Lock();
   if (sync_error) {
       RecordBackgroundError(status);
   }
   }
   if (write_batch == tmp_batch_) tmp_batch_->Clear();
   
   versions_->SetLastSequence(last_sequence);
   }
   
   while (true) {
       Writer* ready = writers_.front();
       writers_.pop_front();
       // 这里表示 这是队长的队员，被连带着完成任务
       if (ready != &w) {
           ready->status = status;
           ready->done = true;
           ready->cv.Signal();
       }
       // 如果是队里的最后一个人，就break
       if (ready == last_writer) break;
   }
   
   // 把队头唤醒！
   if (!writers_.empty()) {
       writers_.front()->cv.Signal();
   }
   ```

   

   1. 假设有w1、w2、w3、w4、w5五个并发的写任务

   2. w1在第二部分代码竞争中获得了锁，成功进入第三部分，发现自己是队头（当前队列唯一的任务），从而顺利地buildbatchgroup

   3. （第五部分）写log和memtable时，可以释放锁，因为w1此时依然为队头，其他写任务进入队列后，只能wait

      假设此时队列为：w1，w3，w5，w4

   4. （第六部分）当w1完成log和memtable写入后，又重新获得锁，w1从队列中pop出来。唤醒队头元素。此时是w3

   5. 【w3、w5、w4】成功buildbatchgroup

   6. （第五部分）释放锁之后，其他写任务继续进入队列

      假设此时队列为：w3，w5，w4，w2

   7. （第六部分）写入log和memtable

   8. 。。。。。一直重复

2. 新问题，在写memtable的时候，为什么不用上锁？（可能有多个读任务在读memtable）

   I/O操作的时候是没有锁的

3. MVCC在leveldb中的实现是怎么样的？

#### 读操作

![image-20241025153601222](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025153601222.png)

对于读操作，怎么获得对应的sequence number呢？

![image-20241025154107172](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025154107172.png)

在进行读操作时，会创建一个快照，当前快照最新的序列号会用作读操作的seq number。

在SSTable中读取数据：`ForEachOverlapping`

```c++
  // 遍历第0层文件
  for (uint32_t i = 0; i < files_[0].size(); i++) {
    FileMetaData* f = files_[0][i];
    if (ucmp->Compare(user_key, f->smallest.user_key()) >= 0 &&
        ucmp->Compare(user_key, f->largest.user_key()) <= 0) {
      tmp.push_back(f);
    }
  }
  if (!tmp.empty()) {
    // 将新的文件放前面
    std::sort(tmp.begin(), tmp.end(), NewestFirst);
    for (uint32_t i = 0; i < tmp.size(); i++) {
      if (!(*func)(arg, 0, tmp[i])) {
        return;
      }
    }
  }

  // Search other levels.
  // 遍历每一层
  for (int level = 1; level < config::kNumLevels; level++) {
    size_t num_files = files_[level].size();
    if (num_files == 0) continue;

    // 二分查找
    uint32_t index = FindFile(vset_->icmp_, files_[level], internal_key);
    if (index < num_files) {
      FileMetaData* f = files_[level][index];
      if (ucmp->Compare(user_key, f->smallest.user_key()) < 0) {
        // All of "f" is past any data for user_key
      } else {
        if (!(*func)(arg, level, f)) {
          return;
        }
      }
    }
  }
```

0层的文件比较特殊。**由于0层的文件中可能存在key重合的情况**，因此在0层中，文件编号大的sstable优先查找。理由是文件编号较大的sstable中存储的总是最新的数据。

非0层文件，**一层中所有文件之间的key不重合**，因此leveldb可以借助sstable的元数据（一个文件中最小与最大的key值）进行快速定位，每一层只需要查找一个sstable文件的内容。

#### compaction

**第一个问题：选谁来合并？**

**size compaction**的触发：

compaction->版本更新->Finalize()判断数据量是否太大->size compaction

```c++
// 版本更新后进行Finalize，计算是否需要compaction
void VersionSet::Finalize(Version* v) {
  // Precomputed best level for next compaction
  int best_level = -1;
  double best_score = -1;

  // 遍历每一层
  for (int level = 0; level < config::kNumLevels - 1; level++) {
    double score;
    if (level == 0) {
      score = v->files_[level].size() /
              static_cast<double>(config::kL0_CompactionTrigger);
    } else {
      // Compute the ratio of current size to size limit.
      const uint64_t level_bytes = TotalFileSize(v->files_[level]);
      score =
          static_cast<double>(level_bytes) / MaxBytesForLevel(options_, level);
    }

    if (score > best_score) {
      best_level = level;
      best_score = score;
    }
  }

  v->compaction_level_ = best_level;
  v->compaction_score_ = best_score;
}
```

对于0层文件，当文件数量超过阈值时触发compaction；对于其他层文件，当文件总大小超过阈值（默认10^iMB）。

对于level i（i >= 1）的情况来说，一个读取最多只会访问一个sstable文件。

**seek compaction**的触发

![image-20241025141733895](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025141733895.png)

每当levelDB要查找key18时，就会在SSTable(k, i + 1)发生`seek miss`。

因此，在FileMetaData结构体中引入了`allowed_seeks`字段，该字段初始为文件大小与16KB的比值，不足100则取100；每次无效seek发生时LevelDB都会将该字段值减1。当某SSTable的`allowed_seeks`减为0时，会触发seek compaction。

为什么是初始文件大小和16KB的比值？

![image-20241025142440609](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025142440609.png)

整体来看，1MB 的数据做 25 次查询和 Compaction 的时间差不多，1 次查询就相当于做 40KB 数据的 Compaction。

LevelDB 将其设为更保守的 16KB，进而一个文件的查询次数阈值设定为 `FileSize / 16KB`。

**compaction的范围**

1. minor compaction范围

   ？暂时没看

2. major compaction范围

   在`PickCompaction()`函数中：

   * 对于Size Compaction，level-i层的SSTable输入根据该level的Compaction Pointer（记录在Version中），选取上次Compaction后的第一个SSTable（如果该层还没发生过Compaction）。这是为了尽可能公平地为Size Compaction选取SSTable，避免某些SSTable永远不会被Compact。
   * 对于Seek Compaction，该方法直接将触发Seek Compaction的SSTable加入到level-i层的输入中。
   * 如果触发Compact的SSTable在level-0，`PickCompaction`方法会将level-0层中所有与该SSTable有overlap的SSTable加入level-0层的输入中。

   扩大范围，在`AddBoundaryInputs()`函数中。为什么？

   * ![image-20241025155341026](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025155341026.png)
   * ![image-20241025155401451](C:/Users/z1382/AppData/Roaming/Typora/typora-user-images/image-20241025155401451.png)
   * 总觉得上面这种情况只会出现在level0？？

   



问题：

1. level i（i>0）是UserKey不相同吗？应该是UserKey不相同，在读任务时，FindFile二分查找SSTable时，传入Internalkey

